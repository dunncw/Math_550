---
title: "Linear Models Final Project"
author: "Cayden dunn, Naomi Woodbury"
date: "2022-11-17"
output: html_document
---

```{r, echo=FALSE}
#set the working directory to the current directory 
setwd('/Users/cindydunn/Desktop/Grad_School/Math_550')

#print the working directory
# getwd()
#print everything in the working directory
# list.files()

# read in data from csv
math_course =read.table("final_project/student/student-mat.csv",sep=";",header=TRUE)
portuguese_course =read.table("final_project/student/student-por.csv",sep=";",header=TRUE)

#combine the two data sets
student_data = rbind(math_course,portuguese_course)

#remove the datasets that are not needed
rm(math_course)
rm(portuguese_course)

# Scale response and predictors with zeros by 1
student_data$G3=(student_data$G3 + 1)
student_data$G2=(student_data$G2 + 1)
student_data$G1=(student_data$G1 + 1)
student_data$absences=(student_data$absences + 1)
student_data$failures=(student_data$failures + 1)
student_data$Medu=(student_data$Medu + 1)
student_data$Fedu=(student_data$Fedu + 1)
```

# Table of Contents

1. [Introduction](#introduction)<br>
2. [Mathematical explanations of the methods/models, assumptions](#mathematical-explanations-of-the-methodsmodels-assumptions)<br>
3. [Data analysis-numerical and graphical results and your interpretations of the results](#data-analysis-numerical-and-graphical-results-and-your-interpretations-of-the-results)<br>
4. [Critics-pros and cons of the model chosen](#critics-pros-and-cons-of-the-model-chosen)<br>
5. [R code](#r-code)<br>
6. [Data Manifest](#data-manifest)<br>
7. [References](#references)<br>

# Introduction

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As Finals season approaches and the semester winds down to and end what subject is more meta than final grades. Our team choose to build a model that could accurately predict grades based on a set of possible predictors. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Our Data set included 1044 Students across two Portuguese schools. The data set contains attributes for each students grades, demographic, social and school related features. Each observation can be pulled from one of two distinct subjects: Mathematics (mat) and Portuguese language (por)(Note 382 of these students overlap as they take both classes). The objective of this model will be to accurately predict G3(final grade (numeric: from 0 to 20, output target)). Note a full manifest can be see at the bottom of this document in the data manifest section([jump to](#data-manifest)). 

# Mathematical explanations of the methods/models, assumptions
(nw)<br>

# Data analysis-numerical and graphical results and your interpretations of the results 
(both - write about your respective portion of the project) <br>
Several approaches will be conducted on the data to generate the best possible model. The approaches are as follows:<br>
1. [Boxcox transformation](#boxcox-transformation)<br>
2. [Inverse transformation](#inverse-transformation)<br>
3. [Variable Selection](#variable-selection)<br>
<br>

## Inital model
(nw)
``` {r, echo=FALSE}
# naomi echo=false makes code not show in output
# Initial model
ilm<-lm(G3 ~., data = student_data)
par(mfrow=c(2,2))
plot(ilm)
```

## Boxcox transformation
(nw)
hello world
```{r, echo=FALSE}
#example of how to show a chat so you can talk about distribution of variables and why we changed things
dage<-density(student_data$failures)
par(mfrow=c(2,2))
plot(dage)
qqnorm(student_data$age, pch = 1, frame = FALSE)
qqline(student_data$age, col = "steelblue", lwd = 2)
boxplot(student_data$age, data=student_data, main="data",ylab="G3"  )

```
hello world

```{r, echo=FALSE}
library(car)

pc<-powerTransform(cbind(student_data$age, student_data$Medu, student_data$Fedu, student_data$traveltime, student_data$studytime, student_data$failures, student_data$famrel, student_data$freetime, student_data$goout, student_data$Dalc, student_data$Walc, student_data$health, student_data$absences,student_data$G1, student_data$G2, student_data$G3)~1)

# Transform numeric predictors
tage<-student_data$age^(-1)
tFedu<-student_data$Fedu^(0.67)
ttraveltime<-student_data$traveltime^(-1.66)
tstudytime<-log(student_data$studytime)
tfailures<-student_data$failures^(-6.22)
tfamrel<-student_data$famrel^(2)
tfreetime<-student_data$freetime^(1)
tgoout<-student_data$goout^(1)
tDalc<-student_data$Dalc^(-2.69)
tWalc<-log(student_data$Walc)
thealth<-student_data$health^(1.33)
tabsences<-student_data$absences^(-0.09)
tG1<-student_data$G1^(1.43)
tG2<-student_data$G2^(1.82)
tG3<-student_data$G3^(2)

# Check model with transformed predictors and response
tlm<-lm(tG3~school+sex+tage+address+famsize+Pstatus+Medu+Fedu+Mjob+Fjob+reason+guardian+ttraveltime+tstudytime+tfailures+schoolsup+famsup+paid+activities+nursery+higher+internet+romantic+tfamrel+tfreetime+tgoout+tDalc+tWalc+thealth+tabsences+tG1+tG2, data = student_data)
par(mfrow=c(2,2))
plot(tlm)
```
 hello wrolds 
 
## Inverse transformation
(nw)

```{r, echo=FALSE}
# Inverse response method using transformed predictors
transy<-lm(G3~school+sex+tage+address+famsize+Pstatus+Medu+Fedu+Mjob+Fjob+reason+guardian+ttraveltime+tstudytime+tfailures+schoolsup+famsup+paid+activities+nursery+higher+internet+romantic+tfamrel+tfreetime+tgoout+tDalc+tWalc+thealth+tabsences+tG1+tG2, data = student_data)
lamy<-invResPlot(transy, lambda=c(-1,-1/2,-1/3,-1/4,0,1/4,1/3,1/2,1))
iG3 <- student_data$G3^(2.019768)

inverse_res_model<-lm(iG3~school+sex+tage+address+famsize+Pstatus+Medu+Fedu+Mjob+Fjob+reason+guardian+ttraveltime+tstudytime+tfailures+schoolsup+famsup+paid+activities+nursery+higher+internet+romantic+tfamrel+tfreetime+tgoout+tDalc+tWalc+thealth+tabsences+tG1+tG2, data = student_data)
par(mfrow=c(2,2))
plot(inverse_res_model)
```

## Variable Selection

### Forwards and Backwards Selection using AIC and BIC and ADJ $R^2$
```{r, echo=FALSE}
#set the working directory to the current directory 
setwd('/Users/cindydunn/Desktop/Grad_School/Math_550')

#print the working directory
# getwd()
#print everything in the working directory
# list.files()

# read in data from csv
math_course =read.table("final_project/student/student-mat.csv",sep=";",header=TRUE)
portuguese_course =read.table("final_project/student/student-por.csv",sep=";",header=TRUE)

#combine the two data sets
student_data = rbind(math_course,portuguese_course)

#remove the datasets that are not needed
rm(math_course)
rm(portuguese_course)

# for chapter 5 we handled data like this go compare. 

# Create a naive baseline model to comapre too
student_model = lm(G3~.,data=student_data)

#define a function called aicc to calculate the AICc
aicc_func <- function(model,p){
  AIC(model)+2*(p+2)*(p+3)/(nrow(student_data)-p-1)
}

# forward selection
library(leaps)

nullmod<-lm(G3~1,data=student_data)
fullmod<-lm(G3~.,data=student_data)

forward_AIC_step<-step(nullmod,scope=list(lower=nullmod, upper=fullmod), direction="forward",trace=0)
forward_BIC_step<-step(nullmod,scope=list(lower=nullmod, upper=fullmod), direction="forward", k=log(nrow(student_data)),trace=0)

Backwards_aic_model <- step(student_model,trace=0)
Backwards_bic_model <- step(student_model, k=log(nrow(student_data)),trace=0)

models = c("forward_AIC_step","forward_BIC_step","Backwards_aic_model","Backwards_bic_model")
# create a vector of the AICs
AICs = c(AIC(forward_AIC_step),AIC(forward_BIC_step),AIC(Backwards_aic_model),AIC(Backwards_bic_model))
# create a vector of the BICs
BICs = c(BIC(forward_AIC_step),BIC(forward_BIC_step),BIC(Backwards_aic_model),BIC(Backwards_bic_model))
# create the vector of the adj R^2s
adj_R2s = c(summary(forward_AIC_step)$adj.r.squared,summary(forward_BIC_step)$adj.r.squared,summary(Backwards_aic_model)$adj.r.squared,summary(Backwards_bic_model)$adj.r.squared)
#create the data frame
model_metrics = data.frame(models,AICs,BICs,adj_R2s)
knitr::kable(model_metrics, format="markdown")

summary(forward_AIC_step)
```
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Interestingly enough the forward selection AIC model and the backwards selection AIC model convergeed on the same model with 6 parameters. Also the same convergence happened when these variable selection methods used BIC as there metric. This can be seen in the scroll able outputs below. <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Despite this the best model of the 4 is the model produced by forward and backward AIC selection as the convereged to the same model with the highest adj $R^2$. This model has 6 predictors. This is the best model of all the forward and backwards selection models. This model will be compared to the other best models from other sections of this report when choosing the final model. 
<br>

#### Forward Selection using AIC

```{r, attr.output='style="max-height: 450px;"', echo=FALSE}
forward_AIC_step<-step(nullmod,scope=list(lower=nullmod, upper=fullmod), direction="forward")
```
#### Backwards Selection using AIC
```{r, attr.output='style="max-height: 450px;"', echo=FALSE}
Backwards_aic_model <- step(student_model)
```
#### Forward Selection using BIC
```{r, attr.output='style="max-height: 450px;"', echo=FALSE}
forward_AIC_step<-step(nullmod,scope=list(lower=nullmod, upper=fullmod), direction="forward", k=log(nrow(student_data)))
```
#### Backwards Selection using BIC
```{r, attr.output='style="max-height: 450px;"', echo=FALSE}
Backwards_aic_model <- step(student_model, k=log(nrow(student_data)))
```
<br>

### Best Subset 
### Selection 


<div style= "float:right;position: relative; top: -80px;">
```{r, echo=FALSE}
library(leaps)

#(a) Identify the optimal model or models based on adj R2 , AIC, AIC , BIC from the approach based on all possible subsets.
regfit.full = regsubsets(G3 ~ ., data = student_data, nvmax = 32)
reg.summary = summary(regfit.full)
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Predictors ",ylab="RSS",type="l")
# mark the minimum
points(which.min(reg.summary$rss),reg.summary$rss[which.min(reg.summary$rss)],col="red",cex=2,pch=20)
plot(reg.summary$adjr2 ,xlab="Number of Predictors ", ylab="Adjusted RSq",type="l")
# mark the maximum
points(which.max(reg.summary$adjr2),reg.summary$adjr2[which.max(reg.summary$adjr2)],col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Predictors ",ylab="Cp", type='l')
# mark the minimum
points(which.min(reg.summary$cp),reg.summary$cp[which.min(reg.summary$cp)],col="red",cex=2,pch=20)
plot(reg.summary$bic ,xlab="Number of Predictors ",ylab="BIC",type='l')
# mark the minimum
points(which.min(reg.summary$bic),reg.summary$bic[which.min(reg.summary$bic)],col="red",cex=2,pch=20)
```
</div>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As seen in the Charts and table below the best subset model is one of the models between 4-12 predictors. The model with 8 predictors has the highest adj r^2. The model with 4 predictors has the lowest BIC. The model with 6 predictors has the lowest AIC and AICc. These are the three models we will be compared in the next section to see which model is the best subset model.

```{r, echo=FALSE}
#build all subset models from 4 - 12 predictors and compare the metrics
# build best subset model with 4 predictors
best_subset_4_model <- lm(G3 ~ failures+paid+absences+G2, data = student_data)

# build best subset model with 5 predictors
best_subset_5_model <- lm(G3 ~ traveltime+failures+paid+absences+G2, data = student_data)

# build best subset model with 6 predictors
best_subset_6_model <- lm(G3 ~ traveltime+failures+paid+absences+G2+famsup, data = student_data)

# build best subset model with 7 predictors
best_subset_7_model <- lm(G3 ~ traveltime+failures+paid+absences+G2+famsup+Fjob, data = student_data)

# build best subset model with 8 predictors
best_subset_8_model = lm(G3~Fjob+traveltime+failures+famsup+paid+famrel+absences+G2, data=student_data)

# build best subset model with 9 predictors
best_subset_9_model = lm(G3~Fjob+traveltime+failures+famsup+paid+famrel+absences+G2+Mjob, data=student_data)

# build best subset model with 10 predictors
best_subset_10_model = lm(G3~Fjob+traveltime+failures+famsup+paid+famrel+absences+G2+Mjob+activities, data=student_data)

# build best subset model with 11 predictors
best_subset_11_model = lm(G3~Fjob+traveltime+failures+famsup+paid+famrel+absences+G2+Mjob+activities+Pstatus, data=student_data)

# build the best subset model with 12 predictors
best_subset_12_model <- lm(G3 ~ Pstatus+Mjob+Fjob+Fjob+traveltime+failures+famsup+paid+activities+famrel+absences+G2 , data = student_data)

p <- c(4,5,6,7,8,9,10,11,12)
best_subset_models <- list(best_subset_4_model, best_subset_5_model, best_subset_6_model, best_subset_7_model, best_subset_8_model, best_subset_9_model, best_subset_10_model, best_subset_11_model, best_subset_12_model)
best_subset_models_adjR2 <- sapply(best_subset_models, function(x) summary(x)$adj.r.squared)
best_subset_models_names <- c("best_subset_4_model", "best_subset_5_model", "best_subset_6_model", "best_subset_7_model", "best_subset_8_model", "best_subset_9_model", "best_subset_10_model", "best_subset_11_model", "best_subset_12_model")
best_subset_models_AIC <- sapply(best_subset_models, AIC)
#calculate AICc for each model
best_subset_models_AICc <- best_subset_models_AIC + (2*p*(p+1))/(nrow(student_data)-p-1)

best_subset_models_BIC <- sapply(best_subset_models, BIC)

# make a data frame and name the columns 
best_subset_models_df <- data.frame(best_subset_models_names, best_subset_models_AIC, best_subset_models_AICc, best_subset_models_BIC, best_subset_models_adjR2)
colnames(best_subset_models_df) <- c("Model", "AIC", "AICc", "BIC", "adjR2")
knitr::kable(best_subset_models_df, format="markdown")

```

#### Best Subset model with 4 Predictors 
```{r,echo=FALSE}
summary(best_subset_4_model)
```
#### Best Subset model with 6 Predictors 
```{r,echo=FALSE}
summary(best_subset_6_model)
```
#### Best Subset model with 8 Predictors 
```{r,echo=FALSE}
summary(best_subset_8_model)
```
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As seen above the model with 6 and 8 predictors have around the same adj $R^2$ value of 0.8346 and 0.8349 respectivly. The best subset model with 6 predictors is a nested model of the best subset model with 8 predictors. As seen above the extra predictors in the model with 8 predictors have insignificant p-values and do not provide anything to the model. Therefore the model with 6 predictors will be chose over the model with 8 predictors to reduce complexity of the model while maintaining performance. <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As seen above the model with 4 and 6 predictors have around the same adj $R^2$ value of 0.8338 and 0.8346 respectivly. The best subset model with 4 predictors is a nested model of the best subset model with 6 predictors. As seen above the extra predictors in the model with 6 predictors have insignificant p-values and do not provide anything to the model. Therefore the model with 4 predictors will be chose over the model with 8 predictors to reduce complexity of the model while maintaining performance. This is the best subset selection model out of all the subset selection models. This model will be compared to the other best models from other sections of this report when choosing the final model. 
<br>

# Critics-pros and cons of the model chosen 
(both) <br>

# R code 
(both) (add this at end) <br>
# start R code raw


# data manifest
### Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:
1 school - student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira) <br>
2 sex - student's sex (binary: "F" - female or "M" - male)<br>
3 age - student's age (numeric: from 15 to 22)<br>
4 address - student's home address type (binary: "U" - urban or "R" - rural)<br>
5 famsize - family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)<br>
6 Pstatus - parent's cohabitation status (binary: "T" - living together or "A" - apart)<br>
7 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)<br>
8 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)<br>
9 Mjob - mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")<br>
10 Fjob - father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")<br>
11 reason - reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")<br>
12 guardian - student's guardian (nominal: "mother", "father" or "other")<br>
13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)<br>
14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)<br>
15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)<br>
16 schoolsup - extra educational support (binary: yes or no)<br>
17 famsup - family educational support (binary: yes or no)<br>
18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)<br>
19 activities - extra-curricular activities (binary: yes or no)<br>
20 nursery - attended nursery school (binary: yes or no)<br>
21 higher - wants to take higher education (binary: yes or no)<br>
22 internet - Internet access at home (binary: yes or no)<br>
23 romantic - with a romantic relationship (binary: yes or no)<br>
24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)<br>
25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)<br>
26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)<br>
27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)<br>
28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)<br>
29 health - current health status (numeric: from 1 - very bad to 5 - very good)<br>
30 absences - number of school absences (numeric: from 0 to 93)<br>

### these grades are related with the course subject, Math or Portuguese:
31 G1 - first period grade (numeric: from 0 to 20) <br>
31 G2 - second period grade (numeric: from 0 to 20)<br>
32 G3 - final grade (numeric: from 0 to 20, output target)<br>

Additional note: there are several (382) students that belong to both datasets. These students can be identified by searching for identical attributes that characterize each student, as shown in the annexed R file.

### where I got this data set from:
https://archive.ics.uci.edu/ml/datasets/student+performance

# References
P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.
